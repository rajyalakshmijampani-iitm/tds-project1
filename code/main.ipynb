{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from IPython.display import clear_output, display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize request counter\n",
    "request_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.github.com/search/users\"\n",
    "token = \"HIDDEN\"\n",
    "\n",
    "headers={\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_users_in_location(location, min_followers=100):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Getting all users\")\n",
    "    \n",
    "    global request_count\n",
    "\n",
    "    users = []\n",
    "    page = 1\n",
    "    per_page = 100  # Maximum allowed by Github\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        params = {\n",
    "            \"q\": f\"location:{location} followers:>{min_followers}\",\n",
    "            \"per_page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "        request_count += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(f\"Requests made: {request_count}\")        \n",
    "        \n",
    "        \n",
    "        # Check for rate limit and handle if reached\n",
    "        if (response.status_code == 403 or response.status_code == 429) and int(response.headers.get(\"X-RateLimit-Remaining\", 1)) == 0:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\"))\n",
    "            wait_time = max(reset_time - time.time(), 0)\n",
    "            print(f\"Rate limit exceeded, sleeping for {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "            continue  # Retry after waiting\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        users.extend(items)\n",
    "        \n",
    "        # Check if there are more pages\n",
    "        if len(items) < per_page:  # Last page might have fewer results\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(3)  # Avoid hitting the rate limit too quickly\n",
    "    \n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_details(username):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Getting details of {username}\")\n",
    "\n",
    "    global request_count\n",
    "    \n",
    "    url = f\"https://api.github.com/users/{username}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    request_count += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Requests made: {request_count}\")\n",
    "    \n",
    "    # Handle rate limit\n",
    "    if (response.status_code == 403 or response.status_code == 429) and int(response.headers.get(\"X-RateLimit-Remaining\", 1)) == 0:\n",
    "        reset_time = int(response.headers.get(\"X-RateLimit-Reset\"))\n",
    "        wait_time = max(reset_time - time.time(), 0)\n",
    "        print(f\"Rate limit exceeded, sleeping for {wait_time} seconds.\")\n",
    "        time.sleep(wait_time)\n",
    "        return get_user_details(username)  # Retry after waiting\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data for {username}: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_company_name(company):\n",
    "    # Trim whitespace, Strip leading @, convert to uppercase\n",
    "    if company:\n",
    "        return company.strip().lstrip('@').upper()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_user_details_and_save(users, filename):\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Header\n",
    "        writer.writerow([\"login\", \"name\", \"company\", \"location\", \"email\", \"hireable\", \"bio\", \n",
    "                         \"public_repos\", \"followers\", \"following\", \"created_at\"])\n",
    "        \n",
    "        for user in users:\n",
    "            username = user[\"login\"]\n",
    "            user_details = get_user_details(username)\n",
    "            if user_details:\n",
    "                \n",
    "                row = [\n",
    "                    user_details.get(\"login\"),\n",
    "                    user_details.get(\"name\"),\n",
    "                    clean_company_name(user_details.get(\"company\")),\n",
    "                    user_details.get(\"location\"),\n",
    "                    user_details.get(\"email\"),\n",
    "                    user_details.get(\"hireable\"),\n",
    "                    user_details.get(\"bio\"),\n",
    "                    user_details.get(\"public_repos\"),\n",
    "                    user_details.get(\"followers\"),\n",
    "                    user_details.get(\"following\"),\n",
    "                    user_details.get(\"created_at\")\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "                time.sleep(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 500 most recently pushed repositories for a given username\n",
    "\n",
    "def get_repo_details(username):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Getting Repo details of {username}\")\n",
    "    \n",
    "    global request_count\n",
    "    \n",
    "    repos = []\n",
    "    page = 1\n",
    "    per_page = 100  # Max per page\n",
    "\n",
    "    while len(repos) < 500:\n",
    "\n",
    "        url = f\"https://api.github.com/users/{username}/repos\"\n",
    "        \n",
    "        params = {\n",
    "            \"sort\": \"pushed\",  # Sort by the last pushed date\n",
    "            \"direction\": \"desc\",  # Get the most recent first\n",
    "            \"per_page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        request_count += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(f\"Requests made: {request_count}\")\n",
    "\n",
    "        # Check for rate limit and handle if reached\n",
    "        if (response.status_code == 403 or response.status_code == 429) and int(response.headers.get(\"X-RateLimit-Remaining\", 1)) == 0:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\"))\n",
    "            wait_time = max(reset_time - time.time(), 0)\n",
    "            print(f\"Rate limit exceeded, sleeping for {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "            continue  # Retry after waiting\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        repos.extend(data)\n",
    "\n",
    "        if len(data) < per_page:\n",
    "            break  # No more repositories to fetch\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(3)  # Avoid hitting the rate limit too quickly\n",
    "\n",
    "    return repos[:500]  # Return only the first 500 repositories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_repo_details_and_save(users, filename):\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Header\n",
    "        writer.writerow([\"login\", \"full_name\", \"created_at\", \"stargazers_count\", \"watchers_count\", \"language\", \"has_projects\", \"has_wiki\", \"license_name\"])\n",
    "        \n",
    "        for user in users:\n",
    "            username = user[\"login\"]\n",
    "            repo_details = get_repo_details(username)\n",
    "            for repo in repo_details:\n",
    "                row = [\n",
    "                    username,\n",
    "                    repo.get(\"full_name\"),\n",
    "                    repo.get(\"created_at\"),\n",
    "                    repo.get(\"stargazers_count\"),\n",
    "                    repo.get(\"watchers_count\"),\n",
    "                    repo.get(\"language\"),\n",
    "                    repo.get(\"has_projects\"),\n",
    "                    repo.get(\"has_wiki\"),\n",
    "                    repo.get(\"license\").get(\"key\") if repo.get(\"license\") else None\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "            time.sleep(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total requests made: 994'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'User data has been saved to users.csv.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Repository data has been saved to repositories.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main execution\n",
    "location = \"Stockholm\"\n",
    "min_followers = 100\n",
    "\n",
    "\n",
    "all_users = get_all_users_in_location(location, min_followers)\n",
    "\n",
    "# Fetch user details and save \n",
    "fetch_user_details_and_save(all_users, 'users.csv')\n",
    "\n",
    "# Fetch repo details and save \n",
    "fetch_repo_details_and_save(all_users, 'repositories.csv')\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(f\"Total requests made: {request_count}\")\n",
    "display(\"User data has been saved to users.csv.\")\n",
    "display(\"Repository data has been saved to repositories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('users.csv')\n",
    "repositories_df = pd.read_csv('repositories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emmabostian,emilk,mpj,hrydgard,eriklindernoren\n"
     ]
    }
   ],
   "source": [
    "top_5_users = users_df.sort_values(by='followers', ascending=False).head(5)\n",
    "top_5_logins = ','.join(top_5_users['login'].tolist())\n",
    "\n",
    "print(top_5_logins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mange,kallepersson,fesplugas,etnt,pirelenito\n"
     ]
    }
   ],
   "source": [
    "earliest_users = users_df.sort_values(by='created_at', ascending=True).head(5)\n",
    "earliest_logins = ','.join(earliest_users['login'].tolist())\n",
    "\n",
    "print(earliest_logins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mit,apache-2.0,other\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with missing license names and count the occurrences\n",
    "popular_licenses = (\n",
    "    repositories_df['license_name']\n",
    "    .dropna()  # Remove missing values\n",
    "    .value_counts()  # Count occurrences of each license\n",
    "    .head(3)  # Select the top 3 most common licenses\n",
    ")\n",
    "\n",
    "top_3_licenses = ','.join(popular_licenses.index.tolist())\n",
    "\n",
    "print(top_3_licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPOTIFY\n"
     ]
    }
   ],
   "source": [
    "most_common_company = users_df['company'].value_counts().idxmax()\n",
    "\n",
    "print(most_common_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript\n"
     ]
    }
   ],
   "source": [
    "most_common_language = repositories_df['language'].dropna().value_counts().idxmax()\n",
    "\n",
    "print(most_common_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeScript\n"
     ]
    }
   ],
   "source": [
    "users_after_2020 = users_df[users_df['created_at'] > '2020-01-01']\n",
    "\n",
    "merged_df = pd.merge(users_after_2020, repositories_df, on='login')\n",
    "\n",
    "language_counts = merged_df['language'].value_counts()\n",
    "\n",
    "second_most_common_language = language_counts.index[1]\n",
    "\n",
    "print(second_most_common_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAML\n"
     ]
    }
   ],
   "source": [
    "average_stars_per_language = repositories_df.groupby('language')['stargazers_count'].mean()\n",
    "\n",
    "highest_average_language = average_stars_per_language.idxmax()\n",
    "\n",
    "print(highest_average_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotify,Mojang,fornwall,joearms,EmbarkStudios\n"
     ]
    }
   ],
   "source": [
    "users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])\n",
    "\n",
    "top_5_leader_strength = users_df.sort_values(by='leader_strength', ascending=False).head(5)\n",
    "\n",
    "top_5_logins = ','.join(top_5_leader_strength['login'].tolist())\n",
    "\n",
    "print(top_5_logins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033207482463390205\n"
     ]
    }
   ],
   "source": [
    "correlation = users_df['followers'].corr(users_df['public_repos'])\n",
    "\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression slope of followers on public repositories is: 0.2169532566541143\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Step 1: Define the independent (X) and dependent (y) variables\n",
    "X = users_df['public_repos']  # Independent variable (number of public repositories)\n",
    "y = users_df['followers']      # Dependent variable (number of followers)\n",
    "\n",
    "# Step 2: Add a constant to the independent variable\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 3: Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 4: Get the slope (coefficient) for public_repos\n",
    "slope = model.params['public_repos']\n",
    "\n",
    "print(f\"The regression slope of followers on public repositories is: {slope}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between projects enabled and wiki enabled is: 0.3747562458571214\n"
     ]
    }
   ],
   "source": [
    "correlation = repositories_df['has_projects'].corr(repositories_df['has_wiki'])\n",
    "\n",
    "print(f\"The correlation between projects enabled and wiki enabled is: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab of Projects Enabled and Wiki Enabled:\n",
      "Wiki Enabled      False  True \n",
      "Projects Enabled              \n",
      "False              1082     49\n",
      "True               5084  29128\n",
      "The correlation between projects enabled and wiki enabled based on proportions is: -1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a cross-tabulation of the two boolean columns\n",
    "crosstab = pd.crosstab(repositories_df['has_projects'], repositories_df['has_wiki'], rownames=['Projects Enabled'], colnames=['Wiki Enabled'])\n",
    "\n",
    "# Step 2: Calculate the correlation from the crosstab\n",
    "print(\"Crosstab of Projects Enabled and Wiki Enabled:\")\n",
    "print(crosstab)\n",
    "\n",
    "# If you want to calculate the correlation in another way:\n",
    "# Calculate proportions\n",
    "proportions = crosstab.div(crosstab.sum(axis=1), axis=0)\n",
    "\n",
    "# Correlation of proportions\n",
    "correlation = proportions.loc[True].corr(proportions.loc[False])\n",
    "\n",
    "print(f\"The correlation between projects enabled and wiki enabled based on proportions is: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average followers for hireable users: 340.3838383838384\n",
      "Average followers for non-hireable users: nan\n",
      "Difference: nan\n"
     ]
    }
   ],
   "source": [
    "average_followers_hireable = users_df[users_df['hireable'] == True]['followers'].mean()\n",
    "\n",
    "average_followers_non_hireable = users_df[users_df['hireable'] == False]['followers'].mean()\n",
    "\n",
    "average_difference = average_followers_hireable - average_followers_non_hireable\n",
    "\n",
    "print(f\"Average followers for hireable users: {average_followers_hireable}\")\n",
    "print(f\"Average followers for non-hireable users: {average_followers_non_hireable}\")\n",
    "print(f\"Difference: {average_difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression slope of followers on bio word count: 6.554\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Filter out users without bios\n",
    "users_df = users_df[users_df['bio'].notnull()]\n",
    "\n",
    "# Calculate the length of each bio in words\n",
    "users_df['bio_word_count'] = users_df['bio'].str.split().str.len()\n",
    "\n",
    "# Prepare the independent variable (X) and dependent variable (y)\n",
    "X = users_df['bio_word_count']\n",
    "y = users_df['followers'] # Adjust the column name as per your dataset\n",
    "\n",
    "# Add a constant to the independent variable (for the intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Get the slope (coefficient of the bio_word_count)\n",
    "slope = model.params['bio_word_count']\n",
    "\n",
    "# Print the regression slope rounded to three decimal places\n",
    "print(f\"\\nRegression slope of followers on bio word count: {slope:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 users who created the most repositories on weekends: HaraldNordgren,Nyholm,lydell,linhduongtuan,LinusU\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert 'created_at' to datetime if it's not already\n",
    "repositories_df['created_at'] = pd.to_datetime(repositories_df['created_at'])\n",
    "\n",
    "# Step 2: Filter for weekend entries (Saturday = 5, Sunday = 6)\n",
    "repositories_df['week_day'] = repositories_df['created_at'].dt.dayofweek\n",
    "weekend_repos = repositories_df[repositories_df['week_day'].isin([5, 6])]\n",
    "\n",
    "# Step 3: Group by user login and count the number of repositories\n",
    "top_users = weekend_repos.groupby('login').size().reset_index(name='repo_count')\n",
    "\n",
    "# Step 4: Sort the users by the number of repositories in descending order\n",
    "top_users_sorted = top_users.sort_values(by='repo_count', ascending=False)\n",
    "\n",
    "top_5_users = top_users_sorted.head(5)\n",
    "\n",
    "top_5_logins = ','.join(top_5_users['login'])\n",
    "\n",
    "print(f\"Top 5 users who created the most repositories on weekends: {top_5_logins}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "hireable_users = users_df[users_df['hireable'] == True]\n",
    "fraction_hireable_with_email = hireable_users['email'].notna().mean()\n",
    "\n",
    "non_hireable_users = users_df[users_df['hireable'] == False]\n",
    "fraction_non_hireable_with_email = non_hireable_users['email'].notna().mean()\n",
    "\n",
    "fraction_difference = fraction_hireable_with_email - fraction_non_hireable_with_email\n",
    "\n",
    "print(fraction_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common surname(s): Gustafsson,Persson\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter out missing names and trim whitespace\n",
    "valid_names = users_df['name'].dropna().str.strip()\n",
    "\n",
    "# Step 2: Extract surnames (last word in the name)\n",
    "surnames = valid_names.str.split().str[-1]  # Get the last word (surname)\n",
    "\n",
    "# Step 3: Count occurrences of each surname\n",
    "surname_counts = surnames.value_counts()\n",
    "\n",
    "# Step 4: Identify the most common surname(s)\n",
    "most_common_surname_count = surname_counts.max()  # Get the highest count\n",
    "most_common_surnames = surname_counts[surname_counts == most_common_surname_count].index.tolist()  # Get all surnames with the highest count\n",
    "\n",
    "# Step 5: Sort surnames alphabetically\n",
    "most_common_surnames.sort()\n",
    "\n",
    "# Prepare output\n",
    "common_surnames_str = ','.join(most_common_surnames)\n",
    "print(f\"Most common surname(s): {common_surnames_str}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
